{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d48702",
   "metadata": {},
   "source": [
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1669eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3574f",
   "metadata": {},
   "source": [
    "read the enviroments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f362ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config() -> dict:\n",
    "    load_dotenv()\n",
    "    host = os.getenv(\"PG_HOST\")\n",
    "    user = os.getenv(\"PG_USER\")\n",
    "    password = os.getenv(\"PG_PASSWORD\")\n",
    "    db_name = os.getenv(\"PG_DB\")\n",
    "    port = os.getenv(\"PG_HOST_PORT\")\n",
    "    url = f\"jdbc:postgresql://{host}:{port}/{db_name}\"\n",
    "    res = {\n",
    "        \"host\" : host,\n",
    "        \"user\" : user,\n",
    "        \"password\" : password,\n",
    "        \"db_name\" : db_name,\n",
    "        \"port\" : port,\n",
    "        \"db_url\" : url\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f11a4",
   "metadata": {},
   "source": [
    "Create the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5d5e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/07 12:35:49 WARN Utils: Your hostname, MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 10.231.32.43 instead (on interface en0)\n",
      "25/11/07 12:35:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/m/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/m/.ivy2.5.2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e85055da-4b72-463e-8a97-6fe0314b2e02;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.4 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 68ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e85055da-4b72-463e-8a97-6fe0314b2e02\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/11/07 12:35:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-practice\")\\\n",
    "            .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"8\")\\\n",
    "            .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb8f02",
   "metadata": {},
   "source": [
    "create the function read_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c0a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(spark : SparkSession, table_name: str)-> DataFrame:\n",
    "    config = get_config()\n",
    "\n",
    "    reader = (spark.read.format(\"jdbc\")\\\n",
    "             .option(\"url\", config[\"db_url\"])\\\n",
    "             .option(\"dbtable\", table_name)\\\n",
    "             .option(\"user\", config[\"user\"])\\\n",
    "             .option(\"password\", config[\"password\"])\\\n",
    "             .option(\"driver\", \"org.postgresql.Driver\"))\n",
    "    return reader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc4133",
   "metadata": {},
   "source": [
    "Create the dataframes of needable tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25eae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_film = read_table(spark, \"film\")\n",
    "df_film_category = read_table(spark, \"film_category\")\n",
    "df_category = read_table(spark, \"category\")\n",
    "df_actor = read_table(spark, \"actor\")\n",
    "df_film_actor = read_table(spark, \"film_actor\")\n",
    "df_inventory = read_table(spark, \"inventory\")\n",
    "df_rental = read_table(spark, 'rental')\n",
    "df_payment = read_table(spark, 'payment')\n",
    "df_customer = read_table(spark, 'customer')\n",
    "df_address = read_table(spark, 'address')\n",
    "df_city = read_table(spark, \"city\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4314f",
   "metadata": {},
   "source": [
    "make the query: Output the number of movies in each category, sorted in descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707f60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|   category|amount|\n",
      "+-----------+------+\n",
      "|      Music|   152|\n",
      "|      Drama|   152|\n",
      "|     Travel|   151|\n",
      "|      Games|   150|\n",
      "|    Foreign|   150|\n",
      "|   Children|   150|\n",
      "|     Sci-Fi|   149|\n",
      "|     Action|   149|\n",
      "|  Animation|   148|\n",
      "|   Classics|   147|\n",
      "|     Family|   147|\n",
      "|        New|   147|\n",
      "|Documentary|   145|\n",
      "|     Sports|   145|\n",
      "|     Comedy|   143|\n",
      "|     Horror|   142|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1 = df_category.alias(\"c\")\\\n",
    "    .join(df_film_category.alias(\"fc\"), F.col(\"c.category_id\") == F.col(\"fc.category_id\"), \"inner\")\\\n",
    "    .join(df_film.alias(\"f\"), F.col(\"f.film_id\") == F.col(\"fc.film_id\"), \"inner\")\\\n",
    "    .groupBy(\"c.name\")\\\n",
    "    .agg(F.count(\"fc.film_id\").alias(\"amount\"))\\\n",
    "    .withColumnRenamed(\"name\", \"category\")\\\n",
    "    .select(\"category\", \"amount\")\\\n",
    "    .orderBy(F.desc(\"amount\"))\n",
    "\n",
    "result1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
